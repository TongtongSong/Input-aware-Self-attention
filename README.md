# Input-aware-Self-attention
Input-aware self-attention for Serialized Multi-Layer Multi-Head Attention
 # Reference
[H.Zhu, K.A.Lee, and H.Li, Serialized Multi-Layer Multi-Head Attention for Neural Speaker Embedding, Interspeech 2021.[paper]](https://arxiv.org/abs/2107.06493)
